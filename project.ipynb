{"cells":[{"cell_type":"markdown","id":"a863076d","metadata":{"id":"a863076d"},"source":["## Part 0: Environment Set Up\n","\n","Run the following cells to load the necessary dependencies and the model Llama 3.2 1b. These should be very similar to the steps in a3."]},{"cell_type":"code","execution_count":1,"id":"KJxr0IbiO5a4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15505,"status":"ok","timestamp":1763952165429,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"},"user_tz":300},"id":"KJxr0IbiO5a4","outputId":"73a1d2a7-c736-4bcf-b35d-c0ef1c62f47e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%pip install tinker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6xpD7Qzs3Eb","executionInfo":{"status":"ok","timestamp":1763952188145,"user_tz":300,"elapsed":11162,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"92676a28-ea91-4044-965e-b0f42852431e"},"id":"o6xpD7Qzs3Eb","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tinker\n","  Downloading tinker-0.5.1-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (4.11.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (8.3.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.23.0->tinker) (0.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tinker) (2.0.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (2.11.10)\n","Requirement already satisfied: rich>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (13.9.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from tinker) (1.3.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from tinker) (2.9.0+cu126)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from tinker) (4.57.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from tinker) (4.15.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->tinker) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (0.16.0)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.23.0->tinker) (4.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (0.4.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->tinker) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->tinker) (2.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.36.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (4.67.1)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (4.1.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->tinker) (1.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->tinker) (0.1.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->tinker) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->tinker) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->tinker) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->tinker) (2.5.0)\n","Downloading tinker-0.5.1-py3-none-any.whl (160 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tinker\n","Successfully installed tinker-0.5.1\n"]}]},{"cell_type":"code","source":["import tinker\n","from google.colab import userdata\n","\n","tinker_api_key = userdata.get('tinker-key')\n","service_client = tinker.ServiceClient(api_key=tinker_api_key)"],"metadata":{"id":"7pBPtEIPs8k8","executionInfo":{"status":"ok","timestamp":1763952200464,"user_tz":300,"elapsed":12315,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}}},"id":"7pBPtEIPs8k8","execution_count":3,"outputs":[]},{"cell_type":"code","source":["base_model = \"meta-llama/Llama-3.2-1B\"\n","\n","training_client = service_client.create_lora_training_client(\n","    base_model=base_model\n",")\n","\n","tokenizer = training_client.get_tokenizer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["4d775b3e986c4fbd983e855105e47f50","9fad6e1cc9c74919ad64ea0e66e977ab","dff6fb83f7cb44b0b19d39a7d4b287c5","9a9009f1fce64be8af7e02a5441d68ff","4a6715cce5cb44488d076f848cdfa289","b7baee38f069481c9d16b9e200d75329","46bb9737c6424d53b1e09e87aa8e458f","6e31e1a1c6364c80a39b67775b6b5aaf","798d61dda66a43779f0ecdbbf5622fe7","915c6b2660ce44d58ea24481842125b4","547496cf7d674f79816411d90e6db194","88cc526dfee0480e8f5a2abbcc512bee","aff917641f2b480eb4b3c8a27ee95f96","e016cf4b1dbc4f939a003ce8d948f408","04056a16af1e4a36bad20edd8caa6603","dce5841936ba4bb8b93aea9cdc585746","92e646c2e24643e7b2fb2b01efba768e","0b0831a29c15467da2d4f220941eb79e","afb7da52cb344aa98fd7fe2fc32be14b","26ac6975816841f59fed4527f7cf1ddb","b2eecd2ec6114a9587831b3c4a5d6b57","9ad48b57ddcf435ba722e0863a5efef4","cd7508ea31664ecca6227474893c8db0","4fcc0cb6d2cb49daac07b7ef8f9fe28b","8a20b5e594cd47df86d1cdd312210e26","5667841516b04dcbb03a9db12b3ceb5c","14e70e0c3bcd4c67b7f0b3bc134a2275","0fd345bad6c5472eb5ab67ac09c186c0","91abd60f66204d94827f2f8afef5bfd3","6c6e83c5a10a4142b6555a6de62ae780","0f7b3b9a91e548af84c44ee5285ca027","3bb0425581af49a79d1cb63a06530752","a750372947984466a35598d929868b3c"]},"id":"JIn_AZwptZdn","executionInfo":{"status":"ok","timestamp":1763952236821,"user_tz":300,"elapsed":36355,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"54bfc987-66da-46aa-879a-7ba4e003c5af"},"id":"JIn_AZwptZdn","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d775b3e986c4fbd983e855105e47f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88cc526dfee0480e8f5a2abbcc512bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7508ea31664ecca6227474893c8db0"}},"metadata":{}}]},{"cell_type":"code","source":["from tinker import types\n","import json\n","\n","# REPLACE WITH YOUR OWN FILE PATH \"/content/drive/{path}\"\n","project_directory = \"/content/drive/MyDrive/2025-2026/NLP/project\"\n","script_type = \"romanized\"\n","\n","#native to english\n","dataset_path = f\"{project_directory}/datasets/{script_type}/train.jsonl\"\n","\n","def process_example(example, tokenizer):\n","    hi = example[\"hi\"].strip()\n","    en = example[\"en\"].strip()\n","\n","    if not en:\n","        return None\n","\n","    # Input is JUST Hindi\n","    prompt = hi\n","\n","    # Tokenization\n","    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n","    prompt_weights = [0] * len(prompt_tokens)\n","\n","    completion_text = en.strip() + \" <end_of_text>\"\n","    completion_tokens = tokenizer.encode(\" \" + en, add_special_tokens=False)\n","    completion_weights = [1] * len(completion_tokens)\n","\n","    tokens = prompt_tokens + completion_tokens\n","    weights = prompt_weights + completion_weights\n","\n","    input_tokens = tokens[:-1]\n","    target_tokens = tokens[1:]\n","    weights = weights[1:]\n","\n","    return types.Datum(\n","        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n","        loss_fn_inputs=dict(\n","            weights=np.array(weights, dtype=np.float32),\n","            target_tokens=np.array(target_tokens, dtype=np.int32)\n","        )\n","    )\n","\n","# Load dataset\n","dataset = []\n","with open(dataset_path, \"r\") as f:\n","    for line in f:\n","        try:\n","            ex = process_example(json.loads(line), tokenizer)\n","            if ex:\n","                dataset.append(ex)\n","        except:\n","            continue\n","\n","print(f\"Loaded {len(dataset)} examples\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ir2VVEkOtb0z","executionInfo":{"status":"ok","timestamp":1763966288812,"user_tz":300,"elapsed":3293,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"0e438c3b-65bb-4005-b95d-8ebd5b40dd77"},"id":"ir2VVEkOtb0z","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 9991 examples\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","\n","learning_rate = 2e-5\n","epochs = 2\n","batch_size = 32  # adjust based on GPU memory\n","\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","\n","    # Iterate in batches\n","    for start_idx in tqdm(range(0, len(dataset), batch_size), desc=\"Training\", unit=\"batch\"):\n","        batch = dataset[start_idx:start_idx + batch_size]\n","\n","        # Forward/backward pass\n","        fwdbwd_future = training_client.forward_backward(batch, \"cross_entropy\")\n","        optim_future = training_client.optim_step(types.AdamParams(learning_rate=learning_rate))\n","\n","        # Wait for results\n","        fwdbwd = fwdbwd_future.result()\n","        optim = optim_future.result()\n","\n","        # Optional: compute weighted loss for monitoring\n","        logprobs_list = [np.array(o[\"logprobs\"].data) for o in fwdbwd.loss_fn_outputs]\n","        logprobs = np.concatenate(logprobs_list)\n","\n","        weights_list = [np.array(d.loss_fn_inputs[\"weights\"].data) for d in batch]\n","        weights_list = [w for w in weights_list if w.size > 0]\n","        weights = np.concatenate(weights_list)\n","\n","        loss = -np.dot(logprobs, weights) / weights.sum()\n","        if (start_idx // batch_size) % 100 == 0:\n","            tqdm.write(\n","                f\"Batch {start_idx//batch_size + 1}/\"\n","                f\"{len(dataset)//batch_size + 1}, Loss: {loss:.4f}\"\n","            )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eknIVFsetc0o","executionInfo":{"status":"ok","timestamp":1763968211415,"user_tz":300,"elapsed":1883260,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"13a5f67f-a9a8-4bbb-db4a-2acf0d670bed"},"id":"eknIVFsetc0o","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n"]},{"output_type":"stream","name":"stderr","text":["Training:   0%|          | 1/313 [00:01<06:06,  1.17s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 1/313, Loss: 1.7397\n"]},{"output_type":"stream","name":"stderr","text":["Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 101/313 [06:11<38:04, 10.78s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 101/313, Loss: 2.1598\n"]},{"output_type":"stream","name":"stderr","text":["Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 201/313 [10:34<11:26,  6.13s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 201/313, Loss: 2.1961\n"]},{"output_type":"stream","name":"stderr","text":["Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 301/313 [14:41<00:13,  1.13s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 301/313, Loss: 2.1143\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [16:18<00:00,  3.13s/batch]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/2\n"]},{"output_type":"stream","name":"stderr","text":["Training:   0%|          | 1/313 [00:08<43:44,  8.41s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 1/313, Loss: 1.6609\n"]},{"output_type":"stream","name":"stderr","text":["Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 101/313 [05:11<16:38,  4.71s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 101/313, Loss: 2.0524\n"]},{"output_type":"stream","name":"stderr","text":["Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 201/313 [09:25<06:17,  3.37s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 201/313, Loss: 2.0985\n"]},{"output_type":"stream","name":"stderr","text":["Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 301/313 [14:12<00:23,  1.97s/batch]"]},{"output_type":"stream","name":"stdout","text":["Batch 301/313, Loss: 2.0298\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [15:04<00:00,  2.89s/batch]\n"]}]},{"cell_type":"code","source":["sampling_client = training_client.save_weights_and_get_sampling_client(\n","    name=\"llama-hi-en-translation\"\n",")"],"metadata":{"id":"GfvJRetTtjTM","executionInfo":{"status":"ok","timestamp":1763968507645,"user_tz":300,"elapsed":46556,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}}},"id":"GfvJRetTtjTM","execution_count":23,"outputs":[]},{"cell_type":"code","source":["from concurrent.futures import ThreadPoolExecutor\n","\n","def translate_one(text):\n","    prompt_tokens = tokenizer.encode(text.strip(), add_special_tokens=True)\n","    model_input = types.ModelInput.from_ints(prompt_tokens)\n","\n","    sampling_params = types.SamplingParams(\n","        max_tokens=60,\n","        temperature=0.2,\n","        stop=[\"\\n\", \"<end_of_text>\"]\n","    )\n","\n","    result = sampling_client.sample(\n","        prompt=model_input,\n","        num_samples=1,\n","        sampling_params=sampling_params\n","    ).result()\n","\n","    decoded = tokenizer.decode(result.sequences[0].tokens)\n","    return decoded.replace(\"<end_of_text>\", \"\").strip()\n"],"metadata":{"id":"yYKGgGTJF9_3","executionInfo":{"status":"ok","timestamp":1763974834013,"user_tz":300,"elapsed":42,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}}},"id":"yYKGgGTJF9_3","execution_count":35,"outputs":[]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n","output_path = f\"{project_directory}/romanized_predictions.jsonl\"\n","\n","# Load lines\n","with open(test_dataset_path, \"r\") as f:\n","    lines = [json.loads(l) for l in f]\n","\n","hi_texts = [ex[\"hi\"].strip() for ex in lines]\n","refs =     [ex[\"en\"].strip() for ex in lines]\n","\n","predictions = []\n","\n","# Number of parallel workers\n","num_workers = 16   # 8‚Äì32 works well\n","with ThreadPoolExecutor(max_workers=num_workers) as executor:\n","    futures = {executor.submit(translate_one, txt): idx for idx, txt in enumerate(hi_texts)}\n","\n","    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating test set\", ncols=100):\n","        i = futures[future]\n","        pred = future.result()\n","\n","        predictions.append({\n","            \"hi\": hi_texts[i],\n","            \"reference\": refs[i],\n","            \"prediction\": pred\n","        })\n","\n","# Save predictions\n","with open(output_path, \"w\") as f:\n","    for p in predictions:\n","        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"Predictions saved to {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QooDhS1QF-vE","executionInfo":{"status":"ok","timestamp":1763975432481,"user_tz":300,"elapsed":593012,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"b9b8c4d1-f134-4b9e-be2b-35a0ab0f5248"},"id":"QooDhS1QF-vE","execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["Translating test set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2507/2507 [09:52<00:00,  4.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Predictions saved to /content/drive/MyDrive/2025-2026/NLP/project/romanized_predictions.jsonl\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Calculate BLEU score for fine-tuned model"],"metadata":{"id":"BgntM5dKI85q"},"id":"BgntM5dKI85q"},{"cell_type":"code","source":["!pip install sacrebleu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYyyfPQXuxkK","executionInfo":{"status":"ok","timestamp":1763975509866,"user_tz":300,"elapsed":9483,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"56102639-0379-4ab6-f153-182853fc874e"},"id":"bYyyfPQXuxkK","execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"]}]},{"cell_type":"code","source":["import sacrebleu\n","\n","refs = []\n","hyps = []\n","\n","with open(output_path, \"r\") as f:\n","    for line in f:\n","        entry = json.loads(line)\n","        hyps.append(entry[\"prediction\"])\n","        refs.append([entry[\"reference\"]])  # sacreBLEU expects list of references\n","\n","bleu = sacrebleu.corpus_bleu(hyps, refs)\n","print(\"BLEU score:\", bleu.score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyK8y1r5uywN","executionInfo":{"status":"ok","timestamp":1763975519294,"user_tz":300,"elapsed":1723,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"5ad09cd4-d165-46dc-cfb7-76731f23f687"},"id":"yyK8y1r5uywN","execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU score: 14.095612952919945\n"]}]},{"cell_type":"markdown","source":["## OLD TINKER CODE BELOW"],"metadata":{"id":"_CCSHy3HFHkl"},"id":"_CCSHy3HFHkl"},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","from tinker import types\n","\n","test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n","output_path = f\"{project_directory}/results/romanized_predictions.jsonl\"\n","\n","predictions = []\n","\n","def translate_hindi_to_english(hindi_text, sampling_client, tokenizer, max_tokens=60, temperature=0.2):\n","    \"\"\"\n","    Translate a single Hindi sentence to English using the trained LoRA model.\n","    \"\"\"\n","    prompt_tokens = tokenizer.encode(hindi_text.strip(), add_special_tokens=True)\n","    model_input = types.ModelInput.from_ints(prompt_tokens)\n","    sampling_params = types.SamplingParams(max_tokens=60, temperature=0.2, stop=[\"<end_of_text>\"])\n","    result = sampling_client.sample(prompt=model_input, sampling_params=sampling_params, num_samples=1).result()\n","    translation = tokenizer.decode(result.sequences[0].tokens)\n","    return translation.replace(\"<end_of_text>\", \"\").strip()\n","\n","# Loop over test dataset\n","for line in tqdm(open(test_dataset_path), desc=\"Generating predictions\"):\n","    example = json.loads(line)\n","    hi = example[\"hi\"].strip()\n","    reference = example[\"en\"].strip()\n","\n","    pred = translate_hindi_to_english(hi, sampling_client, tokenizer, max_tokens=100, temperature=0.0)\n","\n","    predictions.append({\n","        \"hi\": hi,\n","        \"reference\": reference,\n","        \"prediction\": pred\n","    })\n","\n","# Save predictions for BLEU evaluation\n","with open(output_path, \"w\") as f:\n","    for p in predictions:\n","        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"Predictions saved to {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"2-gzOlMZtnrm","executionInfo":{"status":"error","timestamp":1763968675817,"user_tz":300,"elapsed":48275,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"c5b95a09-9531-4581-c823-1286c09d6956"},"id":"2-gzOlMZtnrm","execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating predictions: 34it [00:48,  1.42s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3357421471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_hindi_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     predictions.append({\n","\u001b[0;32m/tmp/ipython-input-3357421471.py\u001b[0m in \u001b[0;36mtranslate_hindi_to_english\u001b[0;34m(hindi_text, sampling_client, tokenizer, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_ints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msampling_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSamplingParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<end_of_text>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end_of_text>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def translate_hindi_to_english(hindi_text, sampling_client, tokenizer, max_tokens=60, temperature=0.2):\n","    \"\"\"\n","    Translate a single Hindi sentence to English using the trained LoRA model.\n","    \"\"\"\n","    prompt_tokens = tokenizer.encode(hindi_text.strip(), add_special_tokens=True)\n","    model_input = types.ModelInput.from_ints(prompt_tokens)\n","    sampling_params = types.SamplingParams(max_tokens=60, temperature=0.2, stop=[\"\\n\", \"<end_of_text>\"])\n","    result = sampling_client.sample(prompt=model_input, sampling_params=sampling_params, num_samples=1).result()\n","    translation = tokenizer.decode(result.sequences[0].tokens)\n","    return translation.replace(\"<end_of_text>\", \"\").strip()"],"metadata":{"id":"xxdlj25ZugGA","executionInfo":{"status":"ok","timestamp":1763968688055,"user_tz":300,"elapsed":6,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}}},"id":"xxdlj25ZugGA","execution_count":25,"outputs":[]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","\n","test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n","batch_size = 16  # Tinker handles multiple prompts per request if desired\n","\n","predictions = []\n","\n","with open(test_dataset_path, \"r\") as f:\n","    lines = f.readlines()\n","\n","for start_idx in tqdm(range(0, len(lines), batch_size), desc=\"Translating test dataset\", ncols=100):\n","    batch_lines = lines[start_idx:start_idx+batch_size]\n","    batch_inputs = [json.loads(line)[\"hi\"].strip() for line in batch_lines]\n","    batch_refs = [json.loads(line)[\"en\"].strip() for line in batch_lines]\n","\n","    # Translate each sentence individually\n","    for hi_text, ref_text in zip(batch_inputs, batch_refs):\n","        pred = translate_hindi_to_english(hi_text, sampling_client, tokenizer)\n","        predictions.append({\n","            \"hi\": hi_text,\n","            \"reference\": ref_text,\n","            \"prediction\": pred\n","        })\n","\n","# Save predictions to file\n","output_path = f\"{project_directory}/romanized_predictions.jsonl\"\n","with open(output_path, \"w\") as f:\n","    for p in predictions:\n","        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"Predictions saved to {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"NB0TmcRPuipC","executionInfo":{"status":"error","timestamp":1763974694623,"user_tz":300,"elapsed":50,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"}},"outputId":"b3369723-249e-42f1-89cf-818ebdbaac81"},"id":"NB0TmcRPuipC","execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["Translating test dataset:   0%|                                             | 0/157 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"SamplingClient.sample() got an unexpected keyword argument 'batch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2691271146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# üöÄ FAST batched translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     batch_preds = batch_translate_hindi_to_english(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msampling_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3157691758.py\u001b[0m in \u001b[0;36mbatch_translate_hindi_to_english\u001b[0;34m(hindi_texts, sampling_client, tokenizer, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# **ONE batched sample call**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     result = sampling_client.sample(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tinker/lib/telemetry.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseverity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: SamplingClient.sample() got an unexpected keyword argument 'batch'"]}]},{"cell_type":"markdown","source":["# PREVIOUS CODE BELOW"],"metadata":{"id":"P15_WCGDtdZW"},"id":"P15_WCGDtdZW"},{"cell_type":"code","execution_count":null,"id":"405e0bae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55558,"status":"ok","timestamp":1763894056507,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"},"user_tz":300},"id":"405e0bae","outputId":"d43200df-2efe-44f5-a94d-7b2a7fd5e620"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n","Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n","\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: fineGrained).\n","The token `COMS W4705 Token` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `COMS W4705 Token`\n"]}],"source":["%pip install huggingface_hub\n","%pip install sacrebleu\n","%pip install -U bitsandbytes\n","!hf auth login"]},{"cell_type":"code","execution_count":null,"id":"b35569dd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336,"referenced_widgets":["7dbf0abdbd9047038618c46d8560f70b","cc98bc4fc4b542a28f9ceb1d9bf70af9","7b271b59cb694e54a540050150a360a7","ddab0246b76b46f2895659bfe6425052","e50b4fdcf25240859ab42a926fb34f20","22223719df5b4646afdc6b09dfbc624c","12198f9b943a45738b7266998f3c0c90","e5ee5a9d14b74e0eb349f0874b9abaa9","22210616c3f44960b1d45fe7d6c33306","8d81bc3af6ee45c4a1e70276dbcd2094","bbdbfc11f5534c15b4a5941eed1c77da","8d67ba5a45dd40f18794f7c7f4290758","8a7ce64eb64245f2aaeb2acc35e5848e","792f9bb340cf4c62a285e732f12ab728","ec6213d50fb24a398a2dd491d3ad4dbc","5647de1945d640479d5206cb13a93c6f","6588527d93c14000a580dac3d4ec1f8c","7f32e277345d415192c5bf17f5b093d4","5fc36a090fee44db9ec64f11693ce86d","895be792debe43348aae18a37e4f5b6e","24c46bbde2de48668974063e190f88fa","d1171c8bd46b4a3a898c05aab164f8ae","35ffe4fb082b4c76a455a25425111ca7","122a0c4f552e467fb03b79b4d1686f64","cf1167e1d7de4e65ab86a4e02956a98f","79b517baadfc4d31b295b9e863482774","f7fbdd7af7d047ddbc5d43bcf898b762","15f67acbd2db4e9b8d83c6631827a019","e7ea3f8d8a504aec8ae272d3aecb2afb","6fe1e1976ae248959033b81f4a50011a","cff299d063fd460ab90d43648bb50f02","69bbd87332594f1f848258de0d640ae2","b9651558d7eb4246a9819191adeae511","e9c6a15abd344edd9c3c8bd2a4b3e10a","a8ba4f09237547b79eba45ff8b04371e","293b973117824a80ba0e1e1574c1a1b5","60cad350be314fad8db078a35b65e14e","0e083792152a4a809df2f501dbe29f70","643f115e757140d3ac4f2e23f3d871a8","936b05fbda954b9d947b585c2bbd037a","e7cc631b193e4203afa815a1399c24d0","bec36571773b4eecb1f76a4252035b0c","436701fe747b466c8583ca92f2e37769","7a54a0483dbb45949084fd50d0f92d6e","c13c758185354b3496202a004caac2e0","4be4613500454023b79ffb5819b42ad5","feda73946a264acf90ca422c87964a59","3b7790305f3d4114bc3830e03f2eaf51","a2b7e38e8f394200b158b667a0f42649","def7cdb0860845ca935a9c91066eff27","2080b8855a504c389c6a266f313f62da","7aa64edaa18e423caef593637a37a21c","1ae1dd03de9d4707954b00aa12bea70b","362d158bcfb34ceaab11511ab284c6d9","24ca77c5739f424f9910209db9933bf1","660b3977b8784c59a6ed4f220eb4d25f","4d4c81be7dab4962b1780da5e493c3a9","a3079b5fe61246709dad3bc9ce3bcdbb","edda0180d8f74de1aea6fba368178e8d","f9e8a6bbda6744ebb303902227a54f07","0ca1391d1fc84a46bad5d158b96b8918","1810aeb62fff437e82c649eeb678894e","47ab5f14d0d94f10a5793c15d6ca86ad","4f32811f20ad43f5b879dfbc4af7e2ce","b2aa6e25fa8741798563af07810e2bbf","5a46da928d1c43ff803fd18fdfe37552"]},"executionInfo":{"elapsed":66868,"status":"ok","timestamp":1763893527842,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"},"user_tz":300},"id":"b35569dd","outputId":"4645ab4a-7a20-4b42-db69-a728c79f02e6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dbf0abdbd9047038618c46d8560f70b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d67ba5a45dd40f18794f7c7f4290758","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35ffe4fb082b4c76a455a25425111ca7","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9c6a15abd344edd9c3c8bd2a4b3e10a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c13c758185354b3496202a004caac2e0","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"660b3977b8784c59a6ed4f220eb4d25f","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer,StoppingCriteria, StoppingCriteriaList\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", dtype=\"auto\", device_map=\"auto\")"]},{"cell_type":"markdown","id":"4a943219","metadata":{"id":"4a943219"},"source":["## Part 0: Baseline Score Evaluation\n","\n","We first get baseline BLEU scores for the model with no changes."]},{"cell_type":"markdown","id":"e49ce041","metadata":{"id":"e49ce041"},"source":["First, load our datasets."]},{"cell_type":"code","execution_count":null,"id":"06140d82","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["ec8119e5f54c4347925d5bfb094d6200","1281226c2ee64b2d8005ac41787211b5","91cdfa832f8046c0a910f2ac08911ab6","503665d27d6d452aa3523e9af6e19cec","581f57efbce04d57a0e456acf821a3c8","835d9b3f1c0849e6b9749e672bf8ac9d","a2a8591610b54e54886bc2b75f7fc92e","c5312a26ccdf40558d6e1d5e023aaed6","b96cb9d8ec5f48adbedccfb4a991f61b","1a1dececf64f4c0db6ab6f637d1919cf","d10d23dc86674d5aa1d8237bdc888098","171f7bdf192540b389c153eb1f5a8e8d","1efe246862664a618551b2b8e0b18579","dd78332c69374aba9406f5b9d1c1ea65","0efbe956615741dca2cd1ba3a401af05","da2958628b5942da8c1042438def1107","b6656db378694a93a4c8d527aa56421e","8e004e8845aa4c5dbf23cf0b1f2c446e","65d673513678456584456b33303e3497","1c821c3a59b24152b5f5e9c65cf77497","058aee71ed4a44e3a23142c9e1fe00fd","a4312ecfc6074145a3297dc6b91175bd","f58bed00ea7a47e498481a550e897a09","be79047b300746679dac1f959438ae84","98836e2f849f466bbf82d34591605b5a","a103742af8334aab8e39f7e564b0439e","70e6953fe76e4692aa5028aae3b45dd5","d0ede88d78cf4c7094586b8eacc09687","279ab19789904fcc9f7fedb261f4e345","d11321c6f37942b9a8877eb119e8750b","a0aa5633d4de4fbeb3720b6de04a0489","a0de01765983477193d5e6fbb5cf23d1","ca25187d20f3430fadd92e6f032cd41e"]},"executionInfo":{"elapsed":10067,"status":"ok","timestamp":1763893952962,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"},"user_tz":300},"id":"06140d82","outputId":"81805b7f-74f5-4d72-94c8-9d8af540b907"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec8119e5f54c4347925d5bfb094d6200","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"171f7bdf192540b389c153eb1f5a8e8d","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f58bed00ea7a47e498481a550e897a09","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","# REPLACE WITH YOUR OWN FILE PATH \"/content/drive/{path}\"\n","project_directory = \"/content/drive/MyDrive/2025-2026/NLP/project\"\n","\n","data_files = {\n","    \"train\": f\"{project_directory}/datasets/native_train.jsonl\",\n","    \"validation\": f\"{project_directory}/datasets/native_val.jsonl\",\n","    \"test\": f\"{project_directory}/datasets/native_test.jsonl\"\n","}\n","\n","ds = load_dataset(\"json\", data_files=data_files)\n","\n","train_ds = ds[\"train\"]\n","val_ds   = ds[\"validation\"]\n","test_ds  = ds[\"test\"]\n"]},{"cell_type":"code","execution_count":null,"id":"GXmxh2lCRThi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234788,"status":"ok","timestamp":1763894747784,"user":{"displayName":"Ann Chengying Li","userId":"02632253864159773974"},"user_tz":300},"id":"GXmxh2lCRThi","outputId":"21e174ed-96a3-470a-bf04-1a452825b1fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["=== BASELINE BLEU (before SFT) ===\n","0.1543677125206915\n"]}],"source":["# --- Ensure tokenizer has a pad token for baseline evaluation ---\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# BASELINE BLEU EVALUATION\n","import sacrebleu\n","import torch\n","\n","def baseline_generate(hindi_sentences, model, tok, max_new_tokens=80):\n","    # No system prompt ‚Äî pure baseline ability\n","    inputs = tok(hindi_sentences, return_tensors=\"pt\", padding=True).to(model.device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        do_sample=False\n","    )\n","    return tok.batch_decode(outputs, skip_special_tokens=True)\n","\n","baseline_preds = []\n","baseline_refs = []\n","\n","# TODO: Determine good test data set size\n","for i in range(5):\n","    ex = test_ds[i]\n","    pred = baseline_generate([ex[\"hi\"]], model, tokenizer)[0]\n","    baseline_preds.append(pred.strip())\n","    baseline_refs.append(ex[\"en\"].strip())\n","\n","baseline_bleu = sacrebleu.corpus_bleu(baseline_preds, [baseline_refs])\n","print(\"=== BASELINE BLEU (before SFT) ===\")\n","print(baseline_bleu.score)"]},{"cell_type":"markdown","id":"LzU5gRV0Rc-f","metadata":{"id":"LzU5gRV0Rc-f"},"source":["## Part 1: Supervised Fine Tuning"]},{"cell_type":"markdown","id":"49befefa","metadata":{"id":"49befefa"},"source":["Tokenizer and prompt prefix."]},{"cell_type":"code","execution_count":null,"id":"8c03540f","metadata":{"id":"8c03540f"},"outputs":[],"source":["# System prompt for SFT\n","PROMPT = (\n","    \"You are a translation assistant. Translate the Hindi text into English. \"\n","    \"Do not add explanations or context. Output only the English translation.\\n\"\n",")\n","\n","# Reuse tokenizer\n","tok = tokenizer\n","tok.padding_side = \"right\"\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","tok.truncation_side = \"left\"\n","\n","MAX_LEN = 400\n","SYS_IDS = tok(PROMPT, add_special_tokens=False)[\"input_ids\"]"]},{"cell_type":"code","execution_count":null,"id":"vnmiRd6GQb3M","metadata":{"id":"vnmiRd6GQb3M"},"outputs":[],"source":["\"\"\"\n","TOKENIZATION FUNCTION\n","\"\"\"\n","\n","def tokenize_batch(batch, include_answer=True):\n","    # Input: Hindi text in column \"hi\"\n","    qs = [q.rstrip() for q in batch[\"hi\"]]\n","    enc_q = tok(qs, add_special_tokens=False, padding=False)\n","\n","    # Target: English translation in column \"en\"\n","    if include_answer:\n","        ans = [a.rstrip() for a in batch[\"en\"]]\n","        enc_a = tok(ans, add_special_tokens=False, padding=False)\n","    else:\n","        enc_a = {\"input_ids\": [[] for _ in qs]}\n","\n","    input_ids_list, prompt_len_list = [], []\n","\n","    for q_ids, a_ids in zip(enc_q[\"input_ids\"], enc_a[\"input_ids\"]):\n","        # prompt + hindi + english + eos\n","        ids = SYS_IDS + q_ids + a_ids + [tok.eos_token_id]\n","\n","        if len(ids) > MAX_LEN:\n","            ids = ids[-MAX_LEN:]\n","\n","        input_ids_list.append(ids)\n","        prompt_len_list.append(len(SYS_IDS) + len(q_ids))\n","\n","    return {\n","        \"input_ids\": input_ids_list,\n","        \"prompt_len\": prompt_len_list,\n","    }"]},{"cell_type":"code","execution_count":null,"id":"cy0SUYbBQiHb","metadata":{"id":"cy0SUYbBQiHb"},"outputs":[],"source":["\"\"\"\n","APPLY TOKENIZATION\n","\"\"\"\n","train_tok = train_ds.map(\n","    tokenize_batch,\n","    batched=True,\n","    batch_size=512,\n","    remove_columns=train_ds.column_names,\n",")\n","\n","val_tok = val_ds.map(\n","    tokenize_batch,\n","    batched=True,\n","    batch_size=512,\n","    remove_columns=val_ds.column_names,\n",")\n","\n","test_tok = test_ds.map(\n","    tokenize_batch,\n","    batched=True,\n","    batch_size=512,\n","    remove_columns=test_ds.column_names,\n",")"]},{"cell_type":"code","execution_count":null,"id":"OTUSKAo-Qory","metadata":{"id":"OTUSKAo-Qory"},"outputs":[],"source":["\"\"\"\n","PROMPT MASKED COLLATOR\n","\"\"\"\n","\n","import torch\n","\n","class PromptMaskedCollator:\n","    def __init__(self, tokenizer, pad_to_multiple_of=8):\n","        self.tok = tokenizer\n","        self.pad_to_multiple_of = pad_to_multiple_of\n","\n","    def __call__(self, features):\n","        prompt_len = torch.tensor([f[\"prompt_len\"] for f in features], dtype=torch.long)\n","        feats = [{k: v for k, v in f.items() if k != \"prompt_len\"} for f in features]\n","\n","        batch = self.tok.pad(\n","            feats,\n","            padding=True,\n","            return_tensors=\"pt\",\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","        )\n","\n","        input_ids = batch[\"input_ids\"]\n","        attn = batch[\"attention_mask\"]\n","\n","        T = input_ids.shape[1]\n","        ar = torch.arange(T).unsqueeze(0)\n","\n","        labels = input_ids.clone()\n","        labels[ar < prompt_len.unsqueeze(1)] = -100\n","        labels[attn == 0] = -100\n","\n","        batch[\"labels\"] = labels\n","        return batch\n","\n","collator = PromptMaskedCollator(tok)\n"]},{"cell_type":"code","execution_count":null,"id":"MUomW6c_Qsz0","metadata":{"id":"MUomW6c_Qsz0"},"outputs":[],"source":["\"\"\"\n","LOAD LORA MODEL\n","\"\"\"\n","\n","from transformers import AutoModelForCausalLM\n","from peft import LoraConfig, get_peft_model\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"meta-llama/Llama-3.2-1B\",\n","    device_map=\"auto\",\n","    torch_dtype=\"auto\",\n","    attn_implementation=\"sdpa\",\n",")\n","\n","model.config.use_cache = False\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"id":"5uVrSlEcQxMk","metadata":{"id":"5uVrSlEcQxMk"},"outputs":[],"source":["\"\"\"\n","TRAIN\n","\"\"\"\n","\n","from transformers import Trainer, TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir=\"./hindi_translation_sft\",\n","    num_train_epochs=2,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    learning_rate=2e-4,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.03,\n","    logging_steps=5,\n","    eval_strategy=\"steps\",\n","    eval_steps=50,\n","    save_steps=300,\n","    save_total_limit=2,\n","    fp16=True,\n","    remove_unused_columns=False,\n","    gradient_checkpointing=True,\n","    group_by_length=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_tok,\n","    eval_dataset=val_tok.select(range(100)),\n","    data_collator=collator,\n",")\n","\n","trainer.train()\n","\n","trainer.save_model()\n","tok.save_pretrained(\"./hindi_translation_sft\")\n"]},{"cell_type":"code","execution_count":null,"id":"CIJ8sOukQ1mm","metadata":{"id":"CIJ8sOukQ1mm"},"outputs":[],"source":["\"\"\"\n","BLEU EVALUATION\n","\"\"\"\n","\n","import sacrebleu\n","\n","def generate_translation(model, tok, hindi_sentences, max_new_tokens=80):\n","    inputs = tok(hindi_sentences, return_tensors=\"pt\", padding=True).to(model.device)\n","    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n","    return tok.batch_decode(outputs, skip_special_tokens=True)\n","\n","preds = []\n","refs = []\n","\n","for ex in test_ds:\n","    pred = generate_translation(model, tok, [ex[\"hi\"]])[0]\n","    preds.append(pred.strip())\n","    refs.append(ex[\"en\"].strip())\n","\n","bleu = sacrebleu.corpus_bleu(preds, [refs])\n","print(\"BLEU:\", bleu.score)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4d775b3e986c4fbd983e855105e47f50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9fad6e1cc9c74919ad64ea0e66e977ab","IPY_MODEL_dff6fb83f7cb44b0b19d39a7d4b287c5","IPY_MODEL_9a9009f1fce64be8af7e02a5441d68ff"],"layout":"IPY_MODEL_4a6715cce5cb44488d076f848cdfa289"}},"9fad6e1cc9c74919ad64ea0e66e977ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7baee38f069481c9d16b9e200d75329","placeholder":"‚Äã","style":"IPY_MODEL_46bb9737c6424d53b1e09e87aa8e458f","value":"tokenizer_config.json:‚Äá"}},"dff6fb83f7cb44b0b19d39a7d4b287c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e31e1a1c6364c80a39b67775b6b5aaf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_798d61dda66a43779f0ecdbbf5622fe7","value":1}},"9a9009f1fce64be8af7e02a5441d68ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_915c6b2660ce44d58ea24481842125b4","placeholder":"‚Äã","style":"IPY_MODEL_547496cf7d674f79816411d90e6db194","value":"‚Äá51.0k/?‚Äá[00:00&lt;00:00,‚Äá1.54MB/s]"}},"4a6715cce5cb44488d076f848cdfa289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7baee38f069481c9d16b9e200d75329":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46bb9737c6424d53b1e09e87aa8e458f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e31e1a1c6364c80a39b67775b6b5aaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"798d61dda66a43779f0ecdbbf5622fe7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"915c6b2660ce44d58ea24481842125b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"547496cf7d674f79816411d90e6db194":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88cc526dfee0480e8f5a2abbcc512bee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aff917641f2b480eb4b3c8a27ee95f96","IPY_MODEL_e016cf4b1dbc4f939a003ce8d948f408","IPY_MODEL_04056a16af1e4a36bad20edd8caa6603"],"layout":"IPY_MODEL_dce5841936ba4bb8b93aea9cdc585746"}},"aff917641f2b480eb4b3c8a27ee95f96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92e646c2e24643e7b2fb2b01efba768e","placeholder":"‚Äã","style":"IPY_MODEL_0b0831a29c15467da2d4f220941eb79e","value":"tokenizer.json:‚Äá"}},"e016cf4b1dbc4f939a003ce8d948f408":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afb7da52cb344aa98fd7fe2fc32be14b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26ac6975816841f59fed4527f7cf1ddb","value":1}},"04056a16af1e4a36bad20edd8caa6603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2eecd2ec6114a9587831b3c4a5d6b57","placeholder":"‚Äã","style":"IPY_MODEL_9ad48b57ddcf435ba722e0863a5efef4","value":"‚Äá9.09M/?‚Äá[00:00&lt;00:00,‚Äá44.6MB/s]"}},"dce5841936ba4bb8b93aea9cdc585746":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92e646c2e24643e7b2fb2b01efba768e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b0831a29c15467da2d4f220941eb79e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afb7da52cb344aa98fd7fe2fc32be14b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"26ac6975816841f59fed4527f7cf1ddb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2eecd2ec6114a9587831b3c4a5d6b57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ad48b57ddcf435ba722e0863a5efef4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd7508ea31664ecca6227474893c8db0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4fcc0cb6d2cb49daac07b7ef8f9fe28b","IPY_MODEL_8a20b5e594cd47df86d1cdd312210e26","IPY_MODEL_5667841516b04dcbb03a9db12b3ceb5c"],"layout":"IPY_MODEL_14e70e0c3bcd4c67b7f0b3bc134a2275"}},"4fcc0cb6d2cb49daac07b7ef8f9fe28b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fd345bad6c5472eb5ab67ac09c186c0","placeholder":"‚Äã","style":"IPY_MODEL_91abd60f66204d94827f2f8afef5bfd3","value":"special_tokens_map.json:‚Äá100%"}},"8a20b5e594cd47df86d1cdd312210e26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c6e83c5a10a4142b6555a6de62ae780","max":73,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f7b3b9a91e548af84c44ee5285ca027","value":73}},"5667841516b04dcbb03a9db12b3ceb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bb0425581af49a79d1cb63a06530752","placeholder":"‚Äã","style":"IPY_MODEL_a750372947984466a35598d929868b3c","value":"‚Äá73.0/73.0‚Äá[00:00&lt;00:00,‚Äá2.07kB/s]"}},"14e70e0c3bcd4c67b7f0b3bc134a2275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fd345bad6c5472eb5ab67ac09c186c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91abd60f66204d94827f2f8afef5bfd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c6e83c5a10a4142b6555a6de62ae780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f7b3b9a91e548af84c44ee5285ca027":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bb0425581af49a79d1cb63a06530752":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a750372947984466a35598d929868b3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}