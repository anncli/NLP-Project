{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a863076d",
      "metadata": {
        "id": "a863076d"
      },
      "source": [
        "## Part 0: Environment Set Up\n",
        "\n",
        "Run the following cells to load the necessary dependencies and the model Llama 3.2 1b. These should be very similar to the steps in a3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KJxr0IbiO5a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "KJxr0IbiO5a4",
        "outputId": "0f8c5cfb-f015-46c4-aace-a7de6f319e03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-44895bb5-b231-4213-9a65-402ba68fcb91\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-44895bb5-b231-4213-9a65-402ba68fcb91\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving datasets.zip to datasets (1).zip\n",
            "Archive:  datasets.zip\n",
            "replace datasets/__MACOSX/._datasets? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/._datasets  \n",
            "replace datasets/__MACOSX/datasets/._native? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/._native  \n",
            "replace datasets/__MACOSX/datasets/._romanized? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/._romanized  \n",
            "replace datasets/datasets/native/train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/native/train.jsonl  \n",
            "replace datasets/__MACOSX/datasets/native/._train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/native/._train.jsonl  \n",
            "replace datasets/datasets/native/test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/native/test.jsonl  \n",
            "replace datasets/__MACOSX/datasets/native/._test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/native/._test.jsonl  \n",
            "replace datasets/datasets/native/val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/native/val.jsonl  \n",
            "replace datasets/__MACOSX/datasets/native/._val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/native/._val.jsonl  \n",
            "replace datasets/datasets/romanized/train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/romanized/train.jsonl  \n",
            "replace datasets/__MACOSX/datasets/romanized/._train.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/romanized/._train.jsonl  \n",
            "replace datasets/datasets/romanized/test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/romanized/test.jsonl  \n",
            "replace datasets/__MACOSX/datasets/romanized/._test.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/romanized/._test.jsonl  \n",
            "replace datasets/datasets/romanized/val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/datasets/romanized/val.jsonl  \n",
            "replace datasets/__MACOSX/datasets/romanized/._val.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: datasets/__MACOSX/datasets/romanized/._val.jsonl  \n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!unzip datasets.zip -d datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tinker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6xpD7Qzs3Eb",
        "outputId": "f47822f0-fd41-4e8e-cab1-50781aa1c4f7"
      },
      "id": "o6xpD7Qzs3Eb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tinker in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (4.11.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (8.3.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.23.0->tinker) (0.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tinker) (2.0.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (2.11.10)\n",
            "Requirement already satisfied: rich>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tinker) (13.9.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from tinker) (1.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from tinker) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from tinker) (4.57.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from tinker) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->tinker) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->httpx[http2]<1,>=0.23.0->tinker) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<1,>=0.23.0->tinker) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->tinker) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->tinker) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->tinker) (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->tinker) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->tinker) (4.67.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.23.0->tinker) (4.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->tinker) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->tinker) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->tinker) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->tinker) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->tinker) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->tinker) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tinker\n",
        "from google.colab import userdata\n",
        "\n",
        "#tinker_api_key = userdata.get('tinker-key')\n",
        "service_client = tinker.ServiceClient(api_key=\"tml-3OPUyKP7NIPpjUvgPWuXNdm4iHTSnYTWZTXIhuID9LB9Gyy5ke2COg20f7Ghl18jBAAAA\")"
      ],
      "metadata": {
        "id": "7pBPtEIPs8k8"
      },
      "id": "7pBPtEIPs8k8",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "training_client = service_client.create_lora_training_client(\n",
        "    base_model=base_model\n",
        ")\n",
        "\n",
        "tokenizer = training_client.get_tokenizer()"
      ],
      "metadata": {
        "id": "JIn_AZwptZdn"
      },
      "id": "JIn_AZwptZdn",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tinker import types\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# REPLACE WITH YOUR OWN FILE PATH \"/content/drive/{path}\"\n",
        "project_directory = \"/content/datasets\"\n",
        "script_type = \"romanized\"\n",
        "\n",
        "#native to english\n",
        "dataset_path = f\"{project_directory}/datasets/{script_type}/train.jsonl\"\n",
        "\n",
        "def process_example(example, tokenizer):\n",
        "    hi = example[\"hi\"].strip()\n",
        "    en = example[\"en\"].strip()\n",
        "\n",
        "    if not en:\n",
        "        return None\n",
        "\n",
        "    # Input is JUST Hindi\n",
        "    prompt = hi\n",
        "\n",
        "    # Tokenization\n",
        "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
        "    prompt_weights = [0] * len(prompt_tokens)\n",
        "\n",
        "    completion_text = en.strip() + \" <end_of_text>\"\n",
        "    completion_tokens = tokenizer.encode(\" \" + en, add_special_tokens=False)\n",
        "    completion_weights = [1] * len(completion_tokens)\n",
        "\n",
        "    tokens = prompt_tokens + completion_tokens\n",
        "    weights = prompt_weights + completion_weights\n",
        "\n",
        "    input_tokens = tokens[:-1]\n",
        "    target_tokens = tokens[1:]\n",
        "    weights = weights[1:]\n",
        "\n",
        "    return types.Datum(\n",
        "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
        "        loss_fn_inputs=dict(\n",
        "            weights=np.array(weights, dtype=np.float32),\n",
        "            target_tokens=np.array(target_tokens, dtype=np.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Load dataset\n",
        "dataset = []\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            ex = process_example(json.loads(line), tokenizer)\n",
        "            if ex:\n",
        "                dataset.append(ex)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"Loaded {len(dataset)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir2VVEkOtb0z",
        "outputId": "52f33d2a-3ecc-4923-838f-e1d624d5849e"
      },
      "id": "ir2VVEkOtb0z",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 9991 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 2e-5\n",
        "epochs = 2\n",
        "batch_size = 32  # adjust based on GPU memory\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Iterate in batches\n",
        "    for start_idx in tqdm(range(0, len(dataset), batch_size), desc=\"Training\", unit=\"batch\"):\n",
        "        batch = dataset[start_idx:start_idx + batch_size]\n",
        "\n",
        "        # Forward/backward pass\n",
        "        fwdbwd_future = training_client.forward_backward(batch, \"cross_entropy\")\n",
        "        optim_future = training_client.optim_step(types.AdamParams(learning_rate=learning_rate))\n",
        "\n",
        "        # Wait for results\n",
        "        fwdbwd = fwdbwd_future.result()\n",
        "        optim = optim_future.result()\n",
        "\n",
        "        # Optional: compute weighted loss for monitoring\n",
        "        logprobs_list = [np.array(o[\"logprobs\"].data) for o in fwdbwd.loss_fn_outputs]\n",
        "        logprobs = np.concatenate(logprobs_list)\n",
        "\n",
        "        weights_list = [np.array(d.loss_fn_inputs[\"weights\"].data) for d in batch]\n",
        "        weights_list = [w for w in weights_list if w.size > 0]\n",
        "        weights = np.concatenate(weights_list)\n",
        "\n",
        "        loss = -np.dot(logprobs, weights) / weights.sum()\n",
        "        if (start_idx // batch_size) % 100 == 0:\n",
        "            tqdm.write(\n",
        "                f\"Batch {start_idx//batch_size + 1}/\"\n",
        "                f\"{len(dataset)//batch_size + 1}, Loss: {loss:.4f}\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eknIVFsetc0o",
        "outputId": "aea09e06-8656-453a-98d5-e11fd3e6f962"
      },
      "id": "eknIVFsetc0o",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/313 [01:08<5:55:32, 68.37s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/313, Loss: 3.3960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      | 101/313 [09:55<06:56,  1.96s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 101/313, Loss: 3.0426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   | 201/313 [14:49<02:41,  1.44s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 201/313, Loss: 2.9542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 301/313 [19:27<00:32,  2.74s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 301/313, Loss: 2.7123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [20:49<00:00,  3.99s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/313 [00:01<06:48,  1.31s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/313, Loss: 2.7115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|█▍        | 45/313 [03:11<23:52,  5.35s/batch]WARNING:tinker._base_client:Retrying due to status code 500. text={\"error\":\"Internal Server Error\",\"message\":\"An unexpected error occurred. Log ID: 40deb88b-e7a2-4f7c-9ee7-98dc17a4efc9\"}\n",
            "Training:  32%|███▏      | 101/313 [06:52<27:40,  7.83s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 101/313, Loss: 2.7126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   | 201/313 [12:39<02:23,  1.28s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 201/313, Loss: 2.6618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 301/313 [17:37<00:20,  1.75s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 301/313, Loss: 2.5065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [18:12<00:00,  3.49s/batch]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_client = training_client.save_weights_and_get_sampling_client(\n",
        "    name=\"llama-hi-en-translation\"\n",
        ")"
      ],
      "metadata": {
        "id": "GfvJRetTtjTM"
      },
      "id": "GfvJRetTtjTM",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def translate_one(text):\n",
        "    prompt_tokens = tokenizer.encode(text.strip(), add_special_tokens=True)\n",
        "    model_input = types.ModelInput.from_ints(prompt_tokens)\n",
        "\n",
        "    sampling_params = types.SamplingParams(\n",
        "        max_tokens=60,\n",
        "        temperature=0.2,\n",
        "        stop=[\"\\n\", \"<end_of_text>\"]\n",
        "    )\n",
        "\n",
        "    result = sampling_client.sample(\n",
        "        prompt=model_input,\n",
        "        num_samples=1,\n",
        "        sampling_params=sampling_params\n",
        "    ).result()\n",
        "\n",
        "    decoded = tokenizer.decode(result.sequences[0].tokens)\n",
        "    return decoded.replace(\"<end_of_text>\", \"\").strip()\n"
      ],
      "metadata": {
        "id": "yYKGgGTJF9_3"
      },
      "id": "yYKGgGTJF9_3",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n",
        "output_path = f\"{project_directory}/romanized_predictions.jsonl\"\n",
        "\n",
        "# Load lines\n",
        "with open(test_dataset_path, \"r\") as f:\n",
        "    lines = [json.loads(l) for l in f]\n",
        "\n",
        "hi_texts = [ex[\"hi\"].strip() for ex in lines]\n",
        "refs =     [ex[\"en\"].strip() for ex in lines]\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Number of parallel workers\n",
        "num_workers = 16   # 8–32 works well\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    futures = {executor.submit(translate_one, txt): idx for idx, txt in enumerate(hi_texts)}\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating test set\", ncols=100):\n",
        "        i = futures[future]\n",
        "        pred = future.result()\n",
        "\n",
        "        predictions.append({\n",
        "            \"hi\": hi_texts[i],\n",
        "            \"reference\": refs[i],\n",
        "            \"prediction\": pred\n",
        "        })\n",
        "\n",
        "# Save predictions\n",
        "with open(output_path, \"w\") as f:\n",
        "    for p in predictions:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QooDhS1QF-vE",
        "outputId": "dc28fa5d-65f0-4e5a-baeb-8fcdd48ba732"
      },
      "id": "QooDhS1QF-vE",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating test set: 100%|█████████████████████████████████████| 2507/2507 [14:25<00:00,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/datasets/romanized_predictions.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate BLEU score for fine-tuned model"
      ],
      "metadata": {
        "id": "BgntM5dKI85q"
      },
      "id": "BgntM5dKI85q"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYyyfPQXuxkK",
        "outputId": "f08c0024-24e4-4a90-99ee-7d1655f474f5"
      },
      "id": "bYyyfPQXuxkK",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "refs = []\n",
        "hyps = []\n",
        "\n",
        "with open(output_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        entry = json.loads(line)\n",
        "        hyps.append(entry[\"prediction\"])\n",
        "        refs.append([entry[\"reference\"]])  # sacreBLEU expects list of references\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, refs)\n",
        "print(\"BLEU score:\", bleu.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyK8y1r5uywN",
        "outputId": "47e11f3f-f432-4ff9-ee25-bab3222add32"
      },
      "id": "yyK8y1r5uywN",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 22.927593651983287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLD TINKER CODE BELOW"
      ],
      "metadata": {
        "id": "_CCSHy3HFHkl"
      },
      "id": "_CCSHy3HFHkl"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from tinker import types\n",
        "\n",
        "test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n",
        "output_path = f\"{project_directory}/results/romanized_predictions.jsonl\"\n",
        "\n",
        "predictions = []\n",
        "\n",
        "def translate_hindi_to_english(hindi_text, sampling_client, tokenizer, max_tokens=60, temperature=0.2):\n",
        "    \"\"\"\n",
        "    Translate a single Hindi sentence to English using the trained LoRA model.\n",
        "    \"\"\"\n",
        "    prompt_tokens = tokenizer.encode(hindi_text.strip(), add_special_tokens=True)\n",
        "    model_input = types.ModelInput.from_ints(prompt_tokens)\n",
        "    sampling_params = types.SamplingParams(max_tokens=60, temperature=0.2, stop=[\"<end_of_text>\"])\n",
        "    result = sampling_client.sample(prompt=model_input, sampling_params=sampling_params, num_samples=1).result()\n",
        "    translation = tokenizer.decode(result.sequences[0].tokens)\n",
        "    return translation.replace(\"<end_of_text>\", \"\").strip()\n",
        "\n",
        "# Loop over test dataset\n",
        "for line in tqdm(open(test_dataset_path), desc=\"Generating predictions\"):\n",
        "    example = json.loads(line)\n",
        "    hi = example[\"hi\"].strip()\n",
        "    reference = example[\"en\"].strip()\n",
        "\n",
        "    pred = translate_hindi_to_english(hi, sampling_client, tokenizer, max_tokens=100, temperature=0.0)\n",
        "\n",
        "    predictions.append({\n",
        "        \"hi\": hi,\n",
        "        \"reference\": reference,\n",
        "        \"prediction\": pred\n",
        "    })\n",
        "\n",
        "# Save predictions for BLEU evaluation\n",
        "with open(output_path, \"w\") as f:\n",
        "    for p in predictions:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2-gzOlMZtnrm",
        "outputId": "c5b95a09-9531-4581-c823-1286c09d6956"
      },
      "id": "2-gzOlMZtnrm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 34it [00:48,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3357421471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_hindi_to_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     predictions.append({\n",
            "\u001b[0;32m/tmp/ipython-input-3357421471.py\u001b[0m in \u001b[0;36mtranslate_hindi_to_english\u001b[0;34m(hindi_text, sampling_client, tokenizer, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_ints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msampling_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSamplingParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<end_of_text>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end_of_text>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_hindi_to_english(hindi_text, sampling_client, tokenizer, max_tokens=60, temperature=0.2):\n",
        "    \"\"\"\n",
        "    Translate a single Hindi sentence to English using the trained LoRA model.\n",
        "    \"\"\"\n",
        "    prompt_tokens = tokenizer.encode(hindi_text.strip(), add_special_tokens=True)\n",
        "    model_input = types.ModelInput.from_ints(prompt_tokens)\n",
        "    sampling_params = types.SamplingParams(max_tokens=60, temperature=0.2, stop=[\"\\n\", \"<end_of_text>\"])\n",
        "    result = sampling_client.sample(prompt=model_input, sampling_params=sampling_params, num_samples=1).result()\n",
        "    translation = tokenizer.decode(result.sequences[0].tokens)\n",
        "    return translation.replace(\"<end_of_text>\", \"\").strip()"
      ],
      "metadata": {
        "id": "xxdlj25ZugGA"
      },
      "id": "xxdlj25ZugGA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_dataset_path = f\"{project_directory}/datasets/{script_type}/test.jsonl\"\n",
        "batch_size = 16  # Tinker handles multiple prompts per request if desired\n",
        "\n",
        "predictions = []\n",
        "\n",
        "with open(test_dataset_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for start_idx in tqdm(range(0, len(lines), batch_size), desc=\"Translating test dataset\", ncols=100):\n",
        "    batch_lines = lines[start_idx:start_idx+batch_size]\n",
        "    batch_inputs = [json.loads(line)[\"hi\"].strip() for line in batch_lines]\n",
        "    batch_refs = [json.loads(line)[\"en\"].strip() for line in batch_lines]\n",
        "\n",
        "    # Translate each sentence individually\n",
        "    for hi_text, ref_text in zip(batch_inputs, batch_refs):\n",
        "        pred = translate_hindi_to_english(hi_text, sampling_client, tokenizer)\n",
        "        predictions.append({\n",
        "            \"hi\": hi_text,\n",
        "            \"reference\": ref_text,\n",
        "            \"prediction\": pred\n",
        "        })\n",
        "\n",
        "# Save predictions to file\n",
        "output_path = f\"{project_directory}/romanized_predictions.jsonl\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for p in predictions:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "NB0TmcRPuipC",
        "outputId": "b3369723-249e-42f1-89cf-818ebdbaac81"
      },
      "id": "NB0TmcRPuipC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating test dataset:   0%|                                             | 0/157 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SamplingClient.sample() got an unexpected keyword argument 'batch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2691271146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 🚀 FAST batched translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     batch_preds = batch_translate_hindi_to_english(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msampling_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3157691758.py\u001b[0m in \u001b[0;36mbatch_translate_hindi_to_english\u001b[0;34m(hindi_texts, sampling_client, tokenizer, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# **ONE batched sample call**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     result = sampling_client.sample(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tinker/lib/telemetry.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseverity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SamplingClient.sample() got an unexpected keyword argument 'batch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREVIOUS CODE BELOW"
      ],
      "metadata": {
        "id": "P15_WCGDtdZW"
      },
      "id": "P15_WCGDtdZW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405e0bae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "405e0bae",
        "outputId": "d43200df-2efe-44f5-a94d-7b2a7fd5e620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `COMS W4705 Token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `COMS W4705 Token`\n"
          ]
        }
      ],
      "source": [
        "%pip install huggingface_hub\n",
        "%pip install sacrebleu\n",
        "%pip install -U bitsandbytes\n",
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b35569dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "7dbf0abdbd9047038618c46d8560f70b",
            "cc98bc4fc4b542a28f9ceb1d9bf70af9",
            "7b271b59cb694e54a540050150a360a7",
            "ddab0246b76b46f2895659bfe6425052",
            "e50b4fdcf25240859ab42a926fb34f20",
            "22223719df5b4646afdc6b09dfbc624c",
            "12198f9b943a45738b7266998f3c0c90",
            "e5ee5a9d14b74e0eb349f0874b9abaa9",
            "22210616c3f44960b1d45fe7d6c33306",
            "8d81bc3af6ee45c4a1e70276dbcd2094",
            "bbdbfc11f5534c15b4a5941eed1c77da",
            "8d67ba5a45dd40f18794f7c7f4290758",
            "8a7ce64eb64245f2aaeb2acc35e5848e",
            "792f9bb340cf4c62a285e732f12ab728",
            "ec6213d50fb24a398a2dd491d3ad4dbc",
            "5647de1945d640479d5206cb13a93c6f",
            "6588527d93c14000a580dac3d4ec1f8c",
            "7f32e277345d415192c5bf17f5b093d4",
            "5fc36a090fee44db9ec64f11693ce86d",
            "895be792debe43348aae18a37e4f5b6e",
            "24c46bbde2de48668974063e190f88fa",
            "d1171c8bd46b4a3a898c05aab164f8ae",
            "35ffe4fb082b4c76a455a25425111ca7",
            "122a0c4f552e467fb03b79b4d1686f64",
            "cf1167e1d7de4e65ab86a4e02956a98f",
            "79b517baadfc4d31b295b9e863482774",
            "f7fbdd7af7d047ddbc5d43bcf898b762",
            "15f67acbd2db4e9b8d83c6631827a019",
            "e7ea3f8d8a504aec8ae272d3aecb2afb",
            "6fe1e1976ae248959033b81f4a50011a",
            "cff299d063fd460ab90d43648bb50f02",
            "69bbd87332594f1f848258de0d640ae2",
            "b9651558d7eb4246a9819191adeae511",
            "e9c6a15abd344edd9c3c8bd2a4b3e10a",
            "a8ba4f09237547b79eba45ff8b04371e",
            "293b973117824a80ba0e1e1574c1a1b5",
            "60cad350be314fad8db078a35b65e14e",
            "0e083792152a4a809df2f501dbe29f70",
            "643f115e757140d3ac4f2e23f3d871a8",
            "936b05fbda954b9d947b585c2bbd037a",
            "e7cc631b193e4203afa815a1399c24d0",
            "bec36571773b4eecb1f76a4252035b0c",
            "436701fe747b466c8583ca92f2e37769",
            "7a54a0483dbb45949084fd50d0f92d6e",
            "c13c758185354b3496202a004caac2e0",
            "4be4613500454023b79ffb5819b42ad5",
            "feda73946a264acf90ca422c87964a59",
            "3b7790305f3d4114bc3830e03f2eaf51",
            "a2b7e38e8f394200b158b667a0f42649",
            "def7cdb0860845ca935a9c91066eff27",
            "2080b8855a504c389c6a266f313f62da",
            "7aa64edaa18e423caef593637a37a21c",
            "1ae1dd03de9d4707954b00aa12bea70b",
            "362d158bcfb34ceaab11511ab284c6d9",
            "24ca77c5739f424f9910209db9933bf1",
            "660b3977b8784c59a6ed4f220eb4d25f",
            "4d4c81be7dab4962b1780da5e493c3a9",
            "a3079b5fe61246709dad3bc9ce3bcdbb",
            "edda0180d8f74de1aea6fba368178e8d",
            "f9e8a6bbda6744ebb303902227a54f07",
            "0ca1391d1fc84a46bad5d158b96b8918",
            "1810aeb62fff437e82c649eeb678894e",
            "47ab5f14d0d94f10a5793c15d6ca86ad",
            "4f32811f20ad43f5b879dfbc4af7e2ce",
            "b2aa6e25fa8741798563af07810e2bbf",
            "5a46da928d1c43ff803fd18fdfe37552"
          ]
        },
        "id": "b35569dd",
        "outputId": "4645ab4a-7a20-4b42-db69-a728c79f02e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dbf0abdbd9047038618c46d8560f70b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d67ba5a45dd40f18794f7c7f4290758",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35ffe4fb082b4c76a455a25425111ca7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9c6a15abd344edd9c3c8bd2a4b3e10a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c13c758185354b3496202a004caac2e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "660b3977b8784c59a6ed4f220eb4d25f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer,StoppingCriteria, StoppingCriteriaList\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", dtype=\"auto\", device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a943219",
      "metadata": {
        "id": "4a943219"
      },
      "source": [
        "## Part 0: Baseline Score Evaluation\n",
        "\n",
        "We first get baseline BLEU scores for the model with no changes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49ce041",
      "metadata": {
        "id": "e49ce041"
      },
      "source": [
        "First, load our datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06140d82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "ec8119e5f54c4347925d5bfb094d6200",
            "1281226c2ee64b2d8005ac41787211b5",
            "91cdfa832f8046c0a910f2ac08911ab6",
            "503665d27d6d452aa3523e9af6e19cec",
            "581f57efbce04d57a0e456acf821a3c8",
            "835d9b3f1c0849e6b9749e672bf8ac9d",
            "a2a8591610b54e54886bc2b75f7fc92e",
            "c5312a26ccdf40558d6e1d5e023aaed6",
            "b96cb9d8ec5f48adbedccfb4a991f61b",
            "1a1dececf64f4c0db6ab6f637d1919cf",
            "d10d23dc86674d5aa1d8237bdc888098",
            "171f7bdf192540b389c153eb1f5a8e8d",
            "1efe246862664a618551b2b8e0b18579",
            "dd78332c69374aba9406f5b9d1c1ea65",
            "0efbe956615741dca2cd1ba3a401af05",
            "da2958628b5942da8c1042438def1107",
            "b6656db378694a93a4c8d527aa56421e",
            "8e004e8845aa4c5dbf23cf0b1f2c446e",
            "65d673513678456584456b33303e3497",
            "1c821c3a59b24152b5f5e9c65cf77497",
            "058aee71ed4a44e3a23142c9e1fe00fd",
            "a4312ecfc6074145a3297dc6b91175bd",
            "f58bed00ea7a47e498481a550e897a09",
            "be79047b300746679dac1f959438ae84",
            "98836e2f849f466bbf82d34591605b5a",
            "a103742af8334aab8e39f7e564b0439e",
            "70e6953fe76e4692aa5028aae3b45dd5",
            "d0ede88d78cf4c7094586b8eacc09687",
            "279ab19789904fcc9f7fedb261f4e345",
            "d11321c6f37942b9a8877eb119e8750b",
            "a0aa5633d4de4fbeb3720b6de04a0489",
            "a0de01765983477193d5e6fbb5cf23d1",
            "ca25187d20f3430fadd92e6f032cd41e"
          ]
        },
        "id": "06140d82",
        "outputId": "81805b7f-74f5-4d72-94c8-9d8af540b907"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec8119e5f54c4347925d5bfb094d6200",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "171f7bdf192540b389c153eb1f5a8e8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f58bed00ea7a47e498481a550e897a09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# REPLACE WITH YOUR OWN FILE PATH \"/content/drive/{path}\"\n",
        "project_directory = \"/content/drive/MyDrive/2025-2026/NLP/project\"\n",
        "\n",
        "data_files = {\n",
        "    \"train\": f\"{project_directory}/datasets/native_train.jsonl\",\n",
        "    \"validation\": f\"{project_directory}/datasets/native_val.jsonl\",\n",
        "    \"test\": f\"{project_directory}/datasets/native_test.jsonl\"\n",
        "}\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "train_ds = ds[\"train\"]\n",
        "val_ds   = ds[\"validation\"]\n",
        "test_ds  = ds[\"test\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GXmxh2lCRThi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXmxh2lCRThi",
        "outputId": "21e174ed-96a3-470a-bf04-1a452825b1fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASELINE BLEU (before SFT) ===\n",
            "0.1543677125206915\n"
          ]
        }
      ],
      "source": [
        "# --- Ensure tokenizer has a pad token for baseline evaluation ---\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# BASELINE BLEU EVALUATION\n",
        "import sacrebleu\n",
        "import torch\n",
        "\n",
        "def baseline_generate(hindi_sentences, model, tok, max_new_tokens=80):\n",
        "    # No system prompt — pure baseline ability\n",
        "    inputs = tok(hindi_sentences, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return tok.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "baseline_preds = []\n",
        "baseline_refs = []\n",
        "\n",
        "# TODO: Determine good test data set size\n",
        "for i in range(5):\n",
        "    ex = test_ds[i]\n",
        "    pred = baseline_generate([ex[\"hi\"]], model, tokenizer)[0]\n",
        "    baseline_preds.append(pred.strip())\n",
        "    baseline_refs.append(ex[\"en\"].strip())\n",
        "\n",
        "baseline_bleu = sacrebleu.corpus_bleu(baseline_preds, [baseline_refs])\n",
        "print(\"=== BASELINE BLEU (before SFT) ===\")\n",
        "print(baseline_bleu.score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LzU5gRV0Rc-f",
      "metadata": {
        "id": "LzU5gRV0Rc-f"
      },
      "source": [
        "## Part 1: Supervised Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49befefa",
      "metadata": {
        "id": "49befefa"
      },
      "source": [
        "Tokenizer and prompt prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c03540f",
      "metadata": {
        "id": "8c03540f"
      },
      "outputs": [],
      "source": [
        "# System prompt for SFT\n",
        "PROMPT = (\n",
        "    \"You are a translation assistant. Translate the Hindi text into English. \"\n",
        "    \"Do not add explanations or context. Output only the English translation.\\n\"\n",
        ")\n",
        "\n",
        "# Reuse tokenizer\n",
        "tok = tokenizer\n",
        "tok.padding_side = \"right\"\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "tok.truncation_side = \"left\"\n",
        "\n",
        "MAX_LEN = 400\n",
        "SYS_IDS = tok(PROMPT, add_special_tokens=False)[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vnmiRd6GQb3M",
      "metadata": {
        "id": "vnmiRd6GQb3M"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TOKENIZATION FUNCTION\n",
        "\"\"\"\n",
        "\n",
        "def tokenize_batch(batch, include_answer=True):\n",
        "    # Input: Hindi text in column \"hi\"\n",
        "    qs = [q.rstrip() for q in batch[\"hi\"]]\n",
        "    enc_q = tok(qs, add_special_tokens=False, padding=False)\n",
        "\n",
        "    # Target: English translation in column \"en\"\n",
        "    if include_answer:\n",
        "        ans = [a.rstrip() for a in batch[\"en\"]]\n",
        "        enc_a = tok(ans, add_special_tokens=False, padding=False)\n",
        "    else:\n",
        "        enc_a = {\"input_ids\": [[] for _ in qs]}\n",
        "\n",
        "    input_ids_list, prompt_len_list = [], []\n",
        "\n",
        "    for q_ids, a_ids in zip(enc_q[\"input_ids\"], enc_a[\"input_ids\"]):\n",
        "        # prompt + hindi + english + eos\n",
        "        ids = SYS_IDS + q_ids + a_ids + [tok.eos_token_id]\n",
        "\n",
        "        if len(ids) > MAX_LEN:\n",
        "            ids = ids[-MAX_LEN:]\n",
        "\n",
        "        input_ids_list.append(ids)\n",
        "        prompt_len_list.append(len(SYS_IDS) + len(q_ids))\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"prompt_len\": prompt_len_list,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cy0SUYbBQiHb",
      "metadata": {
        "id": "cy0SUYbBQiHb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "APPLY TOKENIZATION\n",
        "\"\"\"\n",
        "train_tok = train_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    batch_size=512,\n",
        "    remove_columns=train_ds.column_names,\n",
        ")\n",
        "\n",
        "val_tok = val_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    batch_size=512,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")\n",
        "\n",
        "test_tok = test_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    batch_size=512,\n",
        "    remove_columns=test_ds.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTUSKAo-Qory",
      "metadata": {
        "id": "OTUSKAo-Qory"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PROMPT MASKED COLLATOR\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "class PromptMaskedCollator:\n",
        "    def __init__(self, tokenizer, pad_to_multiple_of=8):\n",
        "        self.tok = tokenizer\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "\n",
        "    def __call__(self, features):\n",
        "        prompt_len = torch.tensor([f[\"prompt_len\"] for f in features], dtype=torch.long)\n",
        "        feats = [{k: v for k, v in f.items() if k != \"prompt_len\"} for f in features]\n",
        "\n",
        "        batch = self.tok.pad(\n",
        "            feats,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "        )\n",
        "\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attn = batch[\"attention_mask\"]\n",
        "\n",
        "        T = input_ids.shape[1]\n",
        "        ar = torch.arange(T).unsqueeze(0)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[ar < prompt_len.unsqueeze(1)] = -100\n",
        "        labels[attn == 0] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "collator = PromptMaskedCollator(tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUomW6c_Qsz0",
      "metadata": {
        "id": "MUomW6c_Qsz0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LOAD LORA MODEL\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5uVrSlEcQxMk",
      "metadata": {
        "id": "5uVrSlEcQxMk"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TRAIN\n",
        "\"\"\"\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./hindi_translation_sft\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=5,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=300,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True,\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok.select(range(100)),\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "tok.save_pretrained(\"./hindi_translation_sft\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CIJ8sOukQ1mm",
      "metadata": {
        "id": "CIJ8sOukQ1mm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BLEU EVALUATION\n",
        "\"\"\"\n",
        "\n",
        "import sacrebleu\n",
        "\n",
        "def generate_translation(model, tok, hindi_sentences, max_new_tokens=80):\n",
        "    inputs = tok(hindi_sentences, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tok.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "preds = []\n",
        "refs = []\n",
        "\n",
        "for ex in test_ds:\n",
        "    pred = generate_translation(model, tok, [ex[\"hi\"]])[0]\n",
        "    preds.append(pred.strip())\n",
        "    refs.append(ex[\"en\"].strip())\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(preds, [refs])\n",
        "print(\"BLEU:\", bleu.score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "name": "project.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}